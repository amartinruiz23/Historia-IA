\documentclass[12pt,a4paper]{article}
\renewcommand{\baselinestretch}{1.05}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}


\usepackage{amsmath,amsthm,verbatim,amssymb,amsfonts,amscd, graphicx}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tkz-fct}
\usetikzlibrary{babel}
\usepackage{pgfplots}
\usepackage{booktabs}
\usepackage{float}
\usepackage{enumitem}
\usepackage{forest}
\usepackage{hyperref}
\usepackage{ragged2e}
\usepackage{parskip}

%Uso de la fuente Source Sans Pro

\usepackage[default]{sourcesanspro}
%\usepackage[T1]{fontenc}

%Controlar la partición de palabras.
\pretolerance=5000
\tolerance=6000

%Simbolo de euro
\usepackage{eurosym} % para el euro

%Definición de monospace para codigo inline y paquete listings para código fuente.
\def\code#1{\texttt{#1}}
\usepackage{listingsutf8}
\lstset{
    %extendedchars=false,
    %inputencoding=utf8
}
\usepackage{color}
\definecolor{grisclarito}{gray}{0.95}

\lstdefinestyle{customc}{
  %belowcaptionskip=1\baselineskip,
  breaklines=true,
  frame=single,
  %xleftmargin=\parindent,
  language=C,
  showstringspaces=false,
  %basicstyle=\ttfamily,
  keywordstyle=\bfseries\color{green!40!black},
  commentstyle=\itshape\color{purple!40!black},
  identifierstyle=\color{blue},
  stringstyle=\color{orange},
  backgroundcolor=\color{grisclarito}
}

\hypersetup{
  colorlinks=true,
  linkcolor=black,
  urlcolor=blue
}

\setlength{\parindent}{0pt}
\topmargin0.0cm
\headheight0.0cm
\headsep0.0cm
\oddsidemargin0.0cm
\textheight23.0cm
\textwidth16.5cm
\footskip1.0cm

\renewcommand*\contentsname{Índice}

\begin{document}
\pgfplotsset{compat=1.15}
\begin{titlepage}
  \centering
  \includegraphics[width=0.6\textwidth]{ugr.png}\par\vspace{1cm}
  {\scshape\large Historia de las Matemáticas \par} \vspace{1cm}
  {\huge\bfseries Problemas éticos y morales de la Inteligencia Artificial \par}
  \vspace{0.4cm}
  {\large\itshape Evolución a lo largo de la Historia\\}
  \vspace{0.6cm}
  {\large\itshape  Fernando de la Hoz Moreno \\ Laura Gómez Garrido \\ Antonio Martín Ruiz  \par} \vspace{1.00cm}
  Curso 2019-2020 \\
  \vfill

  % Bottom of the page
  {\large \today\par}
\end{titlepage}

\tableofcontents
\newpage

\setlength{\parskip}{10pt}

\section{Introducción}

La inteligencia artificial es una de las ramas de la computación que mayor expansión ha sufrido en los últimos años. Los resultados obtenidos mediante las técnicas que ofrece están siendo ampliamente utilizados en gran variedad de campos, y no paran de crecer. El futuro de estas técnicas resulta prometedor al comprobar las mejoras que se obtienen cada año. Sin embargo, los novedosos procedimientos que introduce implican también fuertes dilemas éticos y morales. Los peligros para la sociedad que puedan implicar son muchas veces impredecibles, y en la actualidad contemplamos como empiezan a surgir consecuencias indeseadas de la aplicación de esta tecnología.

En este trabajo exploramos el desarrollo del pensamiento sobre la inteligencia artificial a lo largo de la historia. Realizaremos un repaso sobre las diferentes visiones que se han sucedido sobre qué es la inteligencia artificial desde sus inicios a la actualidad, así como el desarrollo de las diferentes tecnologías que la conforman. A continuación trataremos los principales problemas éticos y morales que el uso de estos sistemas ha ido generando a lo largo de la historia, desde su misma definición y ligados al desarrollo de la tecnología. Por último realizamos un repaso de la situación actual con algunos ejemplos concretos de cómo la inteligencia artificial afecta a nuestra realidad.

\section{¿Qué es la Inteligencia Artificial?}

Existen diferentes visiones sobre qué es la inteligencia artificial. Existe una división entre las que se refieren a proceso mental o razonamiento y las que se refieren a conducta. También se pueden dividir entre las que miden el éxito en función de la fidelidad en la forma de actuar de los humanos, o a un concepto ideal de inteligencia (racionalidad). Diferenciamos por tanto cuatro enfoques:

\begin{itemize}
\item Sistemas que piensan como humanos.
\item Sistemas que actúan como humanos.
\item Sistemas que piensan racionalmente.
\item Sistemas que actúan racionalmente.
\end{itemize}
Veamos cada uno de estos enfoques.

\subsection{Comportamiento humano: el enfoque de la prueba de Turing}

La prueba de Turing, propuesta por Alan Turing (1950)\cite{Turing} se diseñó para proporcionar una definición operacional y satisfactoria de la inteligencia. Consiste en una prueba basada en la incapacidad de diferenciar entre entidades inteligentes y seres humanos. El computador supera la prueba si un evaluado humano no es capaz de distinguir si las respuestas a una serie de preguntas planteadas son de una persona o no. Para ello un computador debe poseer las siguientes capacidades:
\begin{itemize}
\item Procesamiento del lenguaje natural para poder comunicarse.
\item Representación del conocimiento para almacenar lo que conoce o siente.
\item Razonamiento automático para utilizar la información almacenada para. responder preguntas y extraer conclusiones
\item Aprendizaje automático para adaptarse a nuevas circunstancias y detectar y extrapolar patrones.
\end{itemize}

La prueba de Turing evita deliberadamente la interacción física directa entre evaluador y computador, ya que se centra únicamente en la inteligencia. Existe una Prueba Global de Turing que incluye una señal de vídeo para que evaluador valore la capacidad de percepción del evaluado, y la capacidad para el evaluador de pasar objetos físicos. Para superar la prueba global el computador debe estar también dotado de

\begin{itemize}
\item Visión computacional para percibir objetos.
\item Robótica para manipular objetos.
\end{itemize}

La prueba sigue vigente, pero los investigadores de la IA han dedicado poco esfuerzo a la evaluación de sus sistemas con la Prueba de Turing, por creer que es más importante el estudio de los principios en los que se basa la inteligencia que duplicar un ejemplar de la misma.

\subsection{Pensar como un humano: el enfoque del modelo cognitivo}
Para poder decir que un programa dado piensa como un humano es necesario contar con un mecanismo para determinar cómo piensan los humanos. Una vez se cuente con una teoría los suficientemente precisa sobre cómo trabaja la mente, se podrá expresar esa teoría en la forma de un programa de computador. En el campo interdisciplinario de la ciencia cognitiva convergen modelos computacionales de IA y técnicas experimentales de psicología intentado elaborar teorías precisas y verificables sobre el funcionamiento de la mente humana.

En los comienzos de la IA existía una confusión entre las distintas aproximaciones: se argumentaba que un algoritmo resolvía adecuadamente una tarea y que por tanto era un buen modelo de representación humana, o viceversa. La correcta diferenciación ha permitido que tanto IA como ciencia cognitiva se desarrollen más rápidamente. Los dos campos se alimentan entre sí, especialmente en las áreas de visión y lenguaje natural.

\subsection{Pensamiento racional: el enfoque de las leyes del pensamiento}
Los silogismos de Sócrates son el primer intento de codificar la "\emph{manera correcta de pensar}", es decir, un proceso de razonamiento irrefutable. Son el inicio del campo que hoy día conocemos como lógica. En el siglo XIX se desarrolla una notación precisa para definir sentencias sobre elementos del mundo y especificar relaciones entre ellos. La tradición logista dentro del campo de la inteligencia artificial trata de construir sistemas inteligentes a partir de la lógica.

Este enfoque presenta dos obstáculos. No es fácil transformar el conocimiento informal y expresarlo en términos formales que requieren de notación lógica, particularmente cuando el conocimiento que se tiene sobre algo no es total. En segundo lugar, hay una diferencia entre poder resolver el problema y poder hacerlo en la práctica. Incluso problemas con apenas una docena de datos pueden agotar los recursos computacionales de cualquier computador. Estos obstáculos están presentes en todo intento de construir sistemas de razonamiento computacional pero surgieron por primera vez en la tradición lógica.

\subsection{Actuar de forma racional: el enfoque del agente racional}
Un agente es algo que razona, pero los agentes informáticos deben tener otros atributos como estar dotados de controles autónomos, percibir su entorno, persistir un periodo de tiempo prolongado, adaptarse a cambios y ser capaces de alcanzar objetivos diferentes. Un agente racional es aque que actúa con la intención de alcanzar el mejor resultado o, cuando hay incertidumbre, el mejor resultado esperado.

El agente racional no solo se centra en hacer inferencias correctas, sino que además debe llevar a cabo las acciones. Efectuar una inferencia correcta no depende de la racionalidad, ya que hay situaciones en las que no existe una acción correcta y hay que tomar decisiones. Existen formas de actuar racionalmente que no implican inferencia. La racionalidad perfecta (hacer siempre lo correcto) no es siempre posible en entornos complejos. La demanda computacional que esto implica es demasiado grande. Es por ello que podemos contar con que los agentes racionales tienen una racionalidad limitada (actuar adecuadamente cuando no se cuenta con el tiempo suficiente para efectuar todos los cálculos deseables).

\section{Historia de la Inteligencia Artificial}

Hacemos en esta sección un breve repaso a la historia de la inteligencia artificial, lo cual nos servirá para comprender mejor la evolución del pensamiento sobre la disciplina. Según \cite{RussellNorvig} podemos contemplar las siguientes fases.

\subsection{Génesis de la Inteligencia Artificial (1943-1955)}
Warren McCulloch y Walter Pitts (1943) han sido reconocidos como los autores del primer trabajo de IA, partiendo tres fuentes: conocimientos sobre la fisiología básica y funcionamiento de las neuronas en el cerebro, el análisis formal de la lógica proposicional de Russel y Whitehead y la teoría de computación de Turing. Propusieron un modelo constituido por neuronas artificiales y mostraron, por ejemplo, que cualquier función de cómputo podía calcularse mediante alguna red de neuronas interconectadas y que todos los conectores lógicos (and, or, not, etc) se podrían implementar utilizando estructuras de red sencillas. También sugirieron el aprendizaje por parte de unas neuronas adecuadamente definidas.

Donald Hebb (1949) propuso y demostró una sencilla regla de actualización para modificar las intensidades de las conexiones entre neuronas. Actualmente es denominada \emph{Regla de Aprendizaje Hebbiano o de Hebb} y sigue vigente en la actualidad.

Alan Turing articuló primero una visión de la IA en su artículo \emph{Computing Machinery and Intelligence}\cite{Turing} en 1950 donde introdujo la prueba de Turing, el aprendizaje automático, los algoritmos genéricos y el aprendizaje por refuerzo.

\subsection{Nacimiento de la Inteligencia Artificial (1956)}
Fue en el verano de 1956 cuando se decidió el nombre que esta nueva área de la investigación adoptaría: \emph{Inteligencia Artificial}. Esto sucedió durante un taller en Darmouth con duración de dos meses, organizado por John McCarthy, Minsky, Claude Shannon y Nathaniel Rochester que tuvo un total de 10 asistentes entre los cuales se incluían Trenchad More de Princeton, Arthur Samuel de IBM, y Ray Solomoff y Oliver Selfridge del MIT.

Aunque no se hicieron grandes progresos en la investigación conjunta, destacaron particularmente dos investigadores del Carnegie Tech (Universidad de Carnegie Mellon), Allen Newel y Helbert Simon quienes contaban con un programa de razonamiento, el \emph{Teórico Lógico(TL)}. Al término del taller, dicho programa ya era capaz de demostrar gran parte de los teoremas del Capítulo 2 de \emph{Principia Matemática} de Russel y Whitehead, incluso con demostraciones más cortas que las aportadas por los propios autores.

Newell y Simon también desarrollaron un lenguaje de procesamiento de listas, IPL, para poder escribir el TL. No disponían de un compilador y lo tradujeron a código máquina a mano. Para evitar errores, trabajaron en paralelo, diciendo en voz alta números binarios, conforme escribían cada instrucción para asegurarse de que ambos coincidían.

\subsection{Entusiasmo inicial, grandes esperanzas (1952-1969)}
El temprano éxito de Newell y Simon siguió el del sistema general de problemas, o SRGP, el cual, a diferencia del Teórico Lógico, desde un principio de se diseñó para que imitara protocolos de resolución de problemas de los seres humanos. Siendo probablemente, SRGP el primer programa que incorporó el enfoque de "\emph{pensar como un humano}". Este y otros éxitos llevaron a Newel y Simon a a formular en 1976 la famosa hipótesis del \emph{sistema de símbolos físicos} que afirma que \begin{quote}\small un sistema de símbolos físcos tiene los medios suficientes y necesarios para generar una acción inteligente\end{quote}, es decir, que cualquier sistema (humano o máquina) que exhibiese inteligencia debería operar manipulando estructuras de datos compuestas por símbolos.

Arthur Samuel (1952) escribió una serie de programas para el juego de damas que eventualmente aprendieron a jugar hasta el nivel de un amateur, echando por tierra la idea de que los computadores sólo pueden hacer lo que se les dice: su programa aprendió a jugar mejor que su creador.

Herter Gelernter (1959) construyó el demostrador de teoremas de geometría (DTG), capaz de demostrar teoremas que muchos estudiantes de matemáticas podían encontrar muy complejos de resolver.

En 1958, John McCarthy realizó tres grandes contribuciones:
\begin{itemize}
\item Definió el lenguaje de alto nivel Lisp, el segundo lenguaje de programación de alto nivel más antiguo que aún hoy se utiliza, al ser creado un año después que FORTRAN, y que se convertiría en el lenguaje de programación dominante de la IA.
\item Publicó el artículo \emph{Programs with Common Sense}, en el que describía el Generador de Consejos. Este era un programa hipotético que podría considerarse el primer sistema de IA completo y que, a diferencia del TL y DTG, utilizaba conocimiento general del mundo. Era capaz de aceptar nuevos axiomas durante el curso normal de operación, permitiéndole así ser competente en nuevas áreas sin necesidad de reprogramación.
\item Junto a otros compañeros, inventó el tiempo compartido para tratar de solucionar el acceso a los escasos y costosos recursos de cómputo.
\end{itemize}

Minsky supervisó el trabajo de una serie de estudiantes del MIT que eligieron un número de problemas limitados cuya solución pareció requerir inteligencia, los cuales se conocen como \emph{micromundos}. El micromundo más famoso fue el mundo de bloques, que consiste en un mundo de bloques sólidos colocados sobre una mesa y cuya tarea típica es la reordenación de los bloques utilizando la mano de un robot que es capaz de tomar un sólo bloque cada vez.

El trabajo realizado por McCulloch y Pitts con redes neuronales hizo florecer esta área. Winograd y Cowan mostraron en 1963 cómo un gran número de elementos podrían representar un concepto individual de forma colectivo, llevando consigo un aumento proporcional en robustez y paralelismo.

\subsection{Una dosis de realidad (1966-1973)}El primer tipo de problemas surgió porque la mayoría de los primeros programas contaban con poco o ningún conocimiento de las materia objeto de estudio; obtenían resultados gracias a sencillas manipulaciones sintácticas. Una anécdota divertida surge con los problemas que surgieron al tratar de traducir algunos textos entre diferentes lenguajes de forma automática.

El segundo problema fue que muchos de los problemas que se estaban tratando de resolver mediante la IA eran intratables. El optimismo que acompañó el logro de la demostración de teoremas pronto se vio eclipsado cuando los investigadores fracasaron en la demostración de teoremas más de unas pocas decenas de condiciones. \begin{quote}\small El hecho de que, en principio, un programa sea capaz de encontrar una solución no implica que tal programa encierre todos los mecanismos necesarios para encontrar la solución en la práctica.\end{quote}

El tercer obstáculo se derivó de las limitaciones inherentes a las estructuras básicas que se utilizaban en la generación de la conducta inteligente. Por ejemplo, en 1969, en el libro de Minsky y Papert, \emph{Perceptrons}, se demostró que si bien era posible lograr que los perceptrones (una red neuronal simple) aprendieran cualquier cosa que pudiesen representar, su capacidad de representación era muy limitada.

\subsection{Sistemas basados en conocimiento: ¿clave del poder? (1969-1979)}
La resolución de problemas durante la primera década de investigación de la IA se centraba en los mecanismos de búsqueda de propósito general (métodos débiles). La alternativa a los métodos débiles es el uso de conocimiento específico del dominio. Para ello es necesario saber de antemano la correspondiente respuesta al problema.

Uno de los primeros ejemplos fue DENDRAL, programa diseñado para inferir la estructura molecular, a partir de la fórmula molecular y la información del espectrómetro de masas. La primera versión de DENDRAL utilizaba la fuerza bruta para generar todas las posibilidades y ver cual coincidía con la información del espectrómetro de masas, pero era inviable. Consultaron a químicos analíticos y vieron que ellos buscaban patrones de picos conocidos en el espectrómetro para reconocer subestructuras y reducir el número de posibilidades. DENDRAL fue el primer sistema de conocimiento intenso con éxito.

Teniendo en cuenta esta lección investigadores de Standford dieron comienzo al Proyecto de Programación Heurística, PPH, dedicados a determinar como aplicar los sistemas expertos en áreas de la actividad humana.

El siguiente gran esfuerzo se realizó en el área de diagnóstico médico. Diseñaron el programa MYCIN para diagnosticar infecciones sanguíneas. Sus diagnósticos eran tan buenos como los de un experto. Contaba con 450 reglas.

Se distinguía de DENDRAL en dos aspectos principalmente. MYCIN no poseía un modelo teórico desde el cual obtener las reglas, como DENTRAL. Tuvieron que obtenerse a través de entrevistas con expertos. En segundo lugar MYCIN debía reflejar la incertidumbre inherente al conocimiento médico.

También se utilizó el conocimiento del dominio en el área de la comprensión del lenguaje natural, pero no se pudo resolver algunos de los problemas que ya habían aparecido con la traducción automática ocasionados por el análisis sintáctico.

El crecimiento de aplicaciones para solucionar problemas del mundo real provocó el aumento de la demanda de esquemas de representación del conocimiento, como Prolog basado en lógica u otros basados en la noción de marcos de Minsky, mas estructurado y jerárquico.

\subsection{La IA se convierte en una industria (desde 1980 hasta el presente)}

En la década de los 80 se comercializaban cientos de sistemas expertos a empresas las cuales conseguían ahorrarse millones de dolares con estos. EEUU, Japón y Reino Unido hicieron grandes inversiones en IA. La industria de la IA paso de unos pocos de millones de dolares en 1980 a billones de dolares en 1988. Poco después llegó "\emph{el invierno de la IA}" que afectó a muchas empresas que no fueron capaces de desarrollar extravagantes productos prometidos.

\subsection{Regreso de las redes neuronales (desde 1986 hasta el presente)}
La informática abandonó el campo de las redes neuronales a finales de los 70, pero continuó en otros campos como la física o la psicología. El impulso mas fuerte se produjo a mediados de los 80, cuando por lo menos cuatro grupos distintos reinventaron el algoritmo de aprendizaje de retroalimentación. Este se aplicó a problemas de aprendizaje de la informática y la psicología, y la gran difusión que conocieron los resultados suscito un gran entusiasmo.

Aquellos modelos de IA, llamados conexionistas, fueron vistos por algunos como competidores tanto de los modelos simbólicos, como de la aproximación lógica. La tendencia actual es que la aproximación conexionista y simbólica son complementarias.

\subsection{La IA se convierte en una ciencia (desde 1987 hasta el presente)}
En los últimos años se ha producido una revolución tanto en el contenido como en la metodología de trabajo en la IA. Actualmente es mas usual el desarrollo sobre teoría ya existentes que proponer nuevas, tomar como base teoremas y evidencias experimentales más que intuición. La IA se fundó en parte en el marco de una rebelión contra las limitaciones de los campos existentes como la teoría de control o la estadística, y ahora abarca estos campos.

En términos metodológicos, se puede decir, ya forma parte del ámbito de los métodos científicos. Para que se acepten, las hipótesis deben someterse a rigurosos experimentos empíricos, y los resultados deben analizarse estadísticamente para identificar su relevancia.

En años recientes, los modelos de Markov ocultos, han dominado el área de reconocimiento del habla gracias a apoyarse una rigurosa teoría matemática que ha permitido basar las investigaciones en resultados obtenidos durante décadas y en generar los modelos a partir de grandes volúmenes de datos mediante un proceso de aprendizaje.

La utilización de metodologías mejoradas y marcos teóricos en las redes neuronales, ha permitido que alcance un grado de conocimiento comparado con otras técnicas similares en estadística, reconocimiento de patrones y aprendizaje automático. Como resultado de estos desarrollos, la tecnología minería de datos ha generado una gran industria.

A finales de los 80 el formalismo de las redes de Bayes apareció para facilitar la representación eficiente y el razonamiento riguroso en las situaciones con conocimiento incierto. Este enfoque supera muchos problemas de los sistemas de razonamiento probabilístico de los 60 y 70 y ahora domina la investigación de la IA en razonamiento incierto y los sistemas expertos. Esta aproximación facilita el aprendizaje a partir de la experiencia combinando lo mejor de la IA clásica y las redes neuronales.
\subsection{Estado del arte}
La inteligencia artificial está obteniendo grandes resultados en la automatización de una gran variedad de tareas. Veamos algunas de las aplicaciones actuales.

En el reconocimiento de imágenes se están obteniendo resultados mejores que los obtenidos por humanos. En la generación de imágenes son capaces de generar imágenes sintéticas prácticamente indistinguibles de fotografías.

\begin{figure}[h!]
 \centering
 \includegraphics[width=1.0\textwidth]{grafica.jpg}
 \caption{Avance en el reconocimiento de imágenes según el benchmark ImageNet}
\end{figure}

\begin{figure}[h!]
 \centering
 \includegraphics[width=0.7\textwidth]{fotos.jpg}
 \caption{Avance en generación de imágenes sintéticas}
\end{figure}

Los sistemas de inteligencia artificial están obteniendo asombrosos resultados en gran variedad de juegos competitivos, desde el ajedrez al Go y en e-Sports como Dota 2, gracias a técnicas que buscan de manera creativa estrategias exitosas en el largo plazo, apoyándose en objetivos auxiliares y aprendiendo de ejemplos humanos.

Con la generalización de las tecnologías de la información, en especial internet, aparecen grandes cantidades de datos que deben ser tratadas y cuyo tamaño no permite que la utilización de aplicaciones informáticas tradicionales de rpocesamiento de datos. Las técnicas de IA ofrecen nuevos procedimientos para el tratamiento de estos datos. A esta disciplina se la conoce como big data.

Otros campos en los que se empiezan a obtener resultados relevantes son el reconocimiento de voz, la comprensión del lenguaje y la navegación automática de vehículos.

\section{Objeciones de Turing}
En 1950 fue publicado, en la revista Mind, el artículo \emph{Computing Machinery and Intelligence} \cite{Turing} escrito por Alan Turing, el cual se centraba en tratar el tema de la Inteligencia Artificial.

El artículo comienza dando en consideración la pregunta de \emph{"¿Pueden pensar las máquinas?"}, dejando en claro que la principal intención del artículo es discutir esta cuestión. Así es como el documento destaca por dos grandes cosas: la primera mención al público del conocido \emph{Test de Turing} y el planteamiento y respuesta de algunas de las \emph{Objeciones de Turing}. En este apartado nos centraremos en estas últimas.

Estas objeciones son los hipotéticos argumentos que Turing se imaginaba que recibiría como respuesta negativa a la pregunta formulada anteriormente. De esta forma, el autor se dedicó a desmentir cada una de estas objeciones en el mismo artículo. Muchos de estos argumentos son de carácter ético y moral, demostrando la importancia que esta dimensión tiene para la disciplina ya desde su artículo fundacional. A continuación, comentaremos cada una de estas objeciones:

\subsection{La objeción teológica}

\begin{quote}\small El pensamiento es una función del alma inmortal del hombre. Dios ha proporcionado un alma inmortal a todos los hombres y mujeres, pero no así a ningún otro animal, ni tampoco a las máquinas. Por consiguiente, ningún animal o máquina puede pensar.\end{quote}

Aquí deja bastante claro tanto al inicio como al final de su razonamiento lo poco de acuerdo que está con esta opinión religiosa y, pese a ello, trata de rebatirla utilizando argumentos de índole teológica.Su contraargumento se basa la propia suposición que la cita hace sobre el alcance de los poderes de Dios.

Concretamente, la cita afirma que Dios sólo puede conferir de alma inmortal a los hombres y mujeres, cuando Él tiene la libertad de conferir alma a un animal si así lo desea. De la misma forma, si así lo quisiera, Dios podría conferirle a una máquina de un alma inmortal, si así lo deseara. Así, al crear máquinas pensantes, seríamos instrumentos de Su voluntad para crear recintos para las almas que él crea, de la misma forma que si procreásemos hijos.

A día de hoy, esta objeción carece prácticamente de importancia puesto que la sociedad actual tiene un nivel de fe religioso mucho menor del que había en aquella época. En la mayoría de países, la religión no posee valor y pese que aún existen algunas sociedades que le dan una suma importancia a alguna religión concreta, estas tienen una minoría en cuanto al poder de desarrollo en el ámbito científico-técnico por lo que no influencían en gran medida al progreso de la IA.

\subsection{La objeción de la \emph{cabeza en la arena}}
\begin{quote}\small Las consecuencias de que las máquinas pensaran serían demasiado terribles. Esperemos y creamos que no pueden hacerlo.\end{quote}

Aquí comenta que nos gusta pensar que el hombre es, en cierto modo, superior al resto de los seres. Concretamente, nos gustaría poder demostrar que es necesariamente superior, puesto que así no habría peligro de que perdiera su posición dominante. Añade, que no tiene sentido tratar de seguir refutando este argumento y, finaliza bromeando con que quizás quienes sean afines con este pensamiento deberían buscar consuelo en las transmigración de las almas.

Pese a la broma que realiza Turing de que quienes poseen este pensamiento quizás deberían de buscar consuelo en alguna fe religiosa. Resulta que esta objeción sigue poseyendo un gran peso en la actualidad. No son pocos aquellos que se asustan al ver los progresos que la IA va realizando a lo largo de la historia y que en el fondo desean que esta alcance su límite y no pueda avanzar más.

Varios ejemplos de esto nos lo encontramos a lo largo de la historia en la literatura y artes audiovisuales, donde se plantean futuros distópicos en los que no sólo ganan conciencia las máquinas sino que además estas se revelan contra sus creadores: los seres humanos. En este tipo de relatos, nos encontramos varias vertientes y se exploran muchos mundos como el hecho de que un robot no podría revelarse contra un humano, la hipotética guerra entre ambos o el mundo tras la casi extinción humana. Por mencionar algunos ejemplos, tenemos:

\begin{itemize}
\item \emph{Serie de la Fundación}\cite{Asimov} conjunto de libros de ciencia ficción escritos por Isaac Asimov en los años 1942-1957 y 1982-1992. Se trata de una ficción donde los artefactos tecnológicos condiconan la organización social. Estos libros son principalmente conocidos por la definición de las tres leyes de la robótica de Asimov, la cuales se seguirían utilizando por otros muchos autores en diferentes obras a lo largo del tiempo.
\item \emph{Trilogía del Sprawl}\cite{Sprawl} escrita por Willian Gibson y compuesta por tres libros: \emph{Neuromante} (1984),\emph{ Conde Cero} (1986) y \emph{Mona Lisa acelerada} (1988).
\item \emph{Fábulas de robots}\cite{Robots} es una colección de cuentos de ciencia ficción del escritor Stanislaw Lem publicada en 1964. En este mundo, los humanos son seres legendarios a los que los robots aborrecen y temen, pero existen.
\end{itemize}

\subsection{La objeción matemática}
\begin{quote}\small Existen muchos resultados de lógica matemática que pueden utilizarse para demostrar que hay limitaciones al potencial de las máquinas de estado discreto. [...] Este es el resultado matemático: se afirma que prueba que las máquinas adolecen de una incapacidad a la que no se encuentra sujeto el intelecto humano.\end{quote}

La respuesta breve que ofrece Turing a esta afirmación, no es ni más ni menos que durante el propio razonamiento se asume que las limitaciones que les ponen a las máquinas no se aplican a los seres humanos, cuando esto no es necesariamente cierto.

Después de todo, las personas respondemos erróneamente con frecuencia y estos errores no nos hace ser menos humanos. Entonces, ¿por qué si una máquina comete errores que también son cometidos por nosotros, ya afirmamos haber demostrado que no es pensante? Es dicha pregunta la que utiliza Turing para rebatir el anterior argumento "matemático".

Esta objeción, sigue existiendo en la actualidad puesto que en el fondo las bases de esta se encuentran en la propia definición de la inteligencia artificial. No existe una definción formal y matemática de inteligencia, no hay un método a seguir para comprobar que algo es o no pensante. Por esto mismo, no es fácil de demostrar si una máquina sigue o no un comportamiento inteligente de forma explícita y sin lugar a dudas, pese a las múltiples pruebas que se han ido planteando a lo largo de la historia. No podemos desmetir, matemáticamente hablando, que un robot piense cuando no existe ninguna definición formal sobre la que plantear alguna demostración lógica, correcta y sin ambigüedades.

En definitiva, se trata de algo que no se puede afirmar o negar, residiendo ahí el interés en este punto de vista: en la búsqueda de esa veracidad o falsedad.


\subsection{El argumento de la conciencia}
\begin{quote}\small No podremos aceptar que la máquina iguala al cerebro hasta que una máquina pueda escribir un soneto o componer un concierto en respuesta a pensamientos y emociones experimentadas y no mediante una cascada aleatoria de símbolos. (Esto es, no sólo escribir el soneto, sino saber que ha sido escrito.) Ningún mecanismo podría sentir placer por sus éxitos (y no meramente emitir artificialmente una señal, fácil artilugio), experimentar pesar cuando se funden sus válvulas, ni sentirse enternecido por los halagos o miserable por sus errores, ni encantada por el sexo o enfadada o deprimida cuando no consigue lo que desea. — Jefferson, 1949\end{quote}

Aquí, Turing comenta que la única forma de estar seguros de si una máquina piensa o no, es de hecho, \emph{ser} la máquina y sentirse uno mismo pensar; puesto que así podríamos describir estos sentimientos al mundo pero nadie se sentiría justificado por prestar atención. De la misma forma, para saber si un hombre concreto piensa, tendríamos que ser ese hombre en particular.

De esta forma, dos individuos pueden razonar que sólo ellos piensan y el contrario no. Pero pese a ello, en lugar de entrar en bucle infinito sobre cuál de los dos piensa, se tiende a recurrir al convenio de que ambos piensan.

Este debate se sigue manteniendo a día de hoy. El argumento reside en que, admitiendo que una máquina es inteligente de acuerdo a alguna definición, la existencia de alguna diferencia entre el pensamiento de una máquina o el de una persona. Como bien dice, Turing, la única forma de saber si una máquina tiene sentimientos o no es ser esa misma máquina. Entonces, tomando en cuenta que un robot piensa, ¿cuándo podremos decir que ese mismo robot tiene un razonamiento similar o mejor al de un ser humano? Y, tomando como afirmativo la posesión de dicho razonamiento, ¿qué diferencias reales habría entre su existencia y la existencia humana?, ¿cómo serían las interacciones entre humanos e inteligencias artificiales?

Son muchas las preguntas que nacen de esta parte y, de la misma forma que sucede con la objeción de la cabeza en la arena, existen muchos ejemplos de estas en la literatura y otros artes artísticos. Algunas obras, tratan no sólo este argumento sino también otros como el ya mencionado y otros que ya mencionaremos pero por comentar algunas tenemos:

\begin{itemize}
\item \emph{Máquinas como yo}\cite{Maquinas} escrito por Ian McEwan en 2019 y ambientado en un hipotético mundo donde Alan Turing no se ha suicidado y se ha dedicado al desarrolo de la inteligencia artificial, consiguiendo la creación de los primeros seres humanos sintéticos.
\item \emph{¿Sueñan los androides con ovejas eléctricas?}\cite{Androides} de Philip K. Dick y publicado en 1982. Más adelante fue adaptada libremente en la película \emph{Blade Runner} en 1982 por Ridley Scott.
\item \emph{Ghost in the Shell}\cite{Shell} de Masamune Shirow en 1989 y que tuvo como continuaciones \emph{Ghost in the Shell 2: Man/Machine Interface} y \emph{Ghost in the Shell 1.5: Human-Error Processor}.
\end{itemize}

\subsection{Argumentos sobre diversas incapacidades}
Estos argumentos suelen ser de la forma: "\emph{Acepto que puedas hacer que las máquinas hagan todo lo que hasta ahora has mencionado, pero nunca podrás hacer que una de ellas haga X"}.

Aquí, tras dar varios ejemplos sobre cosas que fueron dichas que las máquinas no podrían hacer y que a largo plazo sí podría ser implementado, comenta que decir que una máquina no tiene diversidad de conductas se traduce en decir que no tiene gran capacidad de almacenamiento. Conforme pasa el tiempo, cada vez podemos ver que la falta de almacenamiento no es precisamente un problema del que suelan disponer las máquinas.

Además, compara este argumento con el de Jefferson y lo ve como una forma de disfrazar el argumento de la conciencia.

Si algo sabemos bien, es que nada es del todo imposible. Lo que hoy puede ser una incapacidad mañana podrá realizarse con algo de esfuerzo y al día siguiente será algo que nos parecerá tan sencillo que nos costaría creer que al inicio tendríamos problemas para conseguirlo. Por ello, el decir que una máquina no sería capaz de realizar alguna acción no sirve de mucho puesto que, con el tiempo, podremos ver cómo ese algo, probablemente, si pueda ser capaz de realizarlo. Un ejemplo de esto, podría ser el hecho de surcar los cielos de una forma similar a los pájaros, algo que en su momento se veía como una tarea imposible y para la cual hoy en día existen tantos remedios que permiten su realización.

Y, si ya nos centramos en afirmaciones más emocionales, no podemos evitar darle la razón a Turing y compararlo con la objeción anterior.

\subsection{La objeción de Lady Lovelace}
\begin{quote}\small La máquina no pretende crear nada. Puede hacer lo que sea que sepamos ordenarle. - Ada Lovelace, 1842.\end{quote}

Aquí, Turing comenta que la falta de conocimiento de Lovelace respecto al conocimiento matemático y computacional del tiempo de Turing fue lo que, dentro del contexto de Ada, la llevase a realizar dicha afirmación. Además, nos remite a la sección \emph{"Máquinas que aprenden"} del mismo artículo, donde aborda en mayor profundidad y detalle esta temática.

Si ya en aquella época la principal respuesta que tuvo Alan Turing hacia Ada Lovelace fue su falta de conocimiento respecto al tema, a día de hoy esto se hace aún más marcado. Después de todo, hoy en día las máquinas son capaces de hacer cosas que a nosotros nos cuesta un largo esfuerzo el conseguir y que incluso, a veces, no terminamos de comprender del todo su funcionamiento pero aún así la máquina logra obtener el resultado adecuado.

Como ejemplo, mencionaremos a las redes convolucionadas que aplicadas en imágenes pueden llegar a ser capaces de identificar con exactitud figuras que el ojo humano jamás podría a simple vista o la IA desarrollada por Google que es capaz de crear otras redes neuronales\cite{Google}.

\subsection{El argumento de la continuidad del sistema nervioso}
Turing admite que el sistema nervioso no es una máquina de estado discreto. Un pequeño error en la información acerca de las dimensiones del impulso nervioso que incide en una neurona puede marcar una gran diferencia en las dimensiones del impulso de salida. Podría argüirse que, siendo así, no podemos esperar ser capaces de imitar el comportamiento del sistema nervioso con un sistema de estado discreto.

Es cierto que una máquina de estado discreto debe ser diferente de una máquina continua. Sin embargo, si nos apegamos a las condiciones del Test de Turing, tanto nosotros como Turing afirmamos que el examinador no tendría ninguna ventaja con esta diferencia a la hora de realizar esta prueba. No sólo eso, sino que con el avance de la tecnología, si bien seguimos utilizando tecnología discreta, llega a un punto en el que la diferencia entre lo que emula una máquina discreta y lo que llegaría admitir una máquina continua es tan mínima que es prácticamente inapreciable.

Y, pese a que hayamos alcanzado el límite que la tecnología actual nos permite, eso no limita a los desarrolladores quienes buscan nuevas tecnologías capaces de superar ese límite. Un claro ejemplo de esto lo tendríamos en el desarrollo de la computación cuántica que busca superar las barreras encontradas en la computación clásica al estar alcanzando el mínimo de tamaño de los transistores actuales.


\subsection{El argumento de la informalidad del comportamiento}
\begin{quote}\small No es posible producir un conjunto de reglas que pretenda describir lo que una persona debe hacer en cada grupo de circunstancias concedible. Podría, por ejemplo, haber una regla que dictara que debemos detenernos al ver la luz roja de un semáforo y avanzar cuando la luz cambie a verde. Entonces, ¿qué sucedería si por algún desperfecto ambas aparecieran al mismo tiempo? Tal vez se decidiría que lo más seguro sería detenerse. No obstante, más adelante podría surgir otra dificultad a raíz de esta decisión. Intentar proporcionar reglas de conducta que cubran cualquier eventualidad, incluso las que surjan a partir de las luces de los semáforos, parecería imposible.
\end{quote}

En el caso de que este argumento fuera cierto, entonces deberíamos de ser capaces de descubrir por observación lo suficiente acerca de una máquina para predecir su comportamiento futuro en un tiempo razonable. Por ello, Turing reta a los lectores de su artículo a ser capaces de conseguir estas leyes de comportamiento para una pequeña máquina que implementó en Manchester.

Si bien la respuesta de Turing es totalmente viable y, es más, plantea un problema de alta complejidad incluso en la época actual como argumento nos encontramos que esta objeción es algo que aún a día de hoy se sigue diciendo. No obstante, es utilizada principalmente por aquellos con escasez de conocimiento sobre la inteligencia artificial o cuando implementamos a través de un lenguaje de marcado una serie de reglas para fijar una serie de comportamientos. No hace falta decir mucho para dar a entender que a día de hoy estos casos no se pueden considerar como verdaderas inteligencias artificiales puesto que carecen de lo más importante: la capacidad de aprender.

Una IA actual, primero tomará una decisión y después analizará las consecuencias de esta, de forma que verá si tiene que modificar su comportamiento o no la hora de tomar esta decisión u otras similares en un futuro en un intento por maximizar las opciones acertadas. Es difícil que una verdadera IA cometa varias veces el mismo error y sin buscar soluciones en caso de haberlo identificado, a diferencia del ser humano que es es el único animal capaz de tropezar varias veces con la misma piedra.

\subsection{El argumento de la percepción extrasensorial}
Turing no tiene un argumento viable en contra de estas afirmaciones, en caso de considerarlas ciertas. Además, comenta que en caso de considerarlas ciertas, ya no podríamos considerar que nuestros cuerpos se mueven de acuerdo a las leyes conocidas de la física, ni con algunas otras aún no descubiertas.

Aquí, nuevamente, no podemos sino volver a darle toda la razón a Turing puesto que si admitimos las percepciones extrasensoriales sería lo mismo que decir que todo el conocimiento que tenemos sobre nuestro mundo actual no es sino totalmente erróneo. Pese a que no podamos desmentirlas ni tampoco afirmarlas, esto no nos supone ningún obstáculo con el avance actual de la tecnología y podemos seguir desarrollándola como si nada.

De hecho, esto es precisamente lo que sucede, pues es muy poca gente la que le presta atención a este tipo de argumentos al ocurrir algo muy parecido a lo que ocurría con la objeción teológica: no se trata de algo que la sociedad actual se plantee seriamente o le de una mayor importancia, siendo pocos los que les prestan atención a estos detalles e influyendo mínimamente en los avances.

\section{Singularidad tecnológica}

La singularidad tecnológica consiste en un hipotético punto en el futuro en que el crecimiento tecnológico se convierte en incontrolable e irreversible, lo cual puede resultar en consecuencias impredecibles para la humanidad. En \cite{Speculations} se propone un modelo según el cuál dicha singularidad se alcanza debido a la llamada explosión de inteligencia.

Según el artículo, la investigación en ciencia cognitiva está desligada de la actual investigación en inteligencia artificial debido a que los objetivos conseguidos imitando el funcionamiento del cerebro son menores en comparación con un mucho mayor coste. Con la suficiente inversión se podría conseguir la imitación del cerebro humano. Estaríamos entonces ante una inteligencia artificial general o inteligencia artificial fuerte, una máquina que puede entender o aprender cualquier tarea intelectual que un humano pueda realizar. Ello, unido a la escabilidad que ofrece la computación en cuestión de memoria ya supone una importante mejora frente al pensamiento humano.

Una inteligencia artificial general sería capaz de realizar recursivas auto-mejoras, rápidamente llegando al surgimiento de la máquina ultrainteligente, esto es, una maquina capaz de superar todas las actividades intelectuales de cualquier humano. Puesto que el diseño de inteligencias artificiales es una actividad intelectual humana, una máquina ultrainteligente sería capaz de diseñar máquinas mejores. La aceleración sucesiva de estas mejoras es la llamada explosión de inteligencia. La inteligencia humana sería pronto superada por la de estas máquinas.

Aunque pueda parecer una perspectiva fantasiosa, el actual desarrollo de la ciencia cognitiva y la computación no nos dejan lejos de las especulaciones realizadas a mediados del siglo XX. Existe una preocupación real en la comunidad científica sobre esta posibilidad. En 2000 se funda el Singularity Institute of Artificial Intelligente, actualmente renombrado como Machine Intelligence Research Institute, una institución dedicada a la investigación sobre inteligencia artificial y centrado en la identificación y control de posibles problemas para la humanidad causados por la aparición de superinteligencias.

\section{Peligros no intencionados. Seguridad en IA}

Un accidente es una situación en la cual el diseñador humano tenía en mente cierto objetivo o tarea pero el sistema diseñado para dicha tarea produce resultados inesperados o dañinos. Podemos clasificar problemas de seguridad según en qué momento del proceso se producen errores.

Primeramente, el diseñador especifica de manera incorrecta la función objetivo, la cual produce resultados dañinos. Si se establece como único objetivo una tarea específica ignorando otros aspectos del ambiente que pueden ser dañinos si son afectados pueden generarse efectos secundarios negativos. Una definición incorrecta del objetivo puede también generar un saboteo del sistema de recompensas (\emph{reward hacking}) de manera que formalmente se maximice el objetivo indicado pero esto no produzca el efecto deseado.

El diseñador conoce la función objetivo correcta pero la manera de evaluarla es demasiado costosa. Malas extrapolaciones de muestras limitadas pueden generar comportamientos dañinos. Supongamos un agente cuyo objetivo es lograr mover una caja de un lado a otro de la habitación. A veces la manera más efectiva de lograr el objetivo implica hacer algo no relacionado y destructivo para el resto del medio ambiente, como en este caso podría ser derribar un jarrón que se encuentra por el camino.

Podríamos diseñar el agente para darle una recompensa negativa por tocar el jarrón y habríamos terminado. Pero, ¿qué pasaría si hubiera muchas cosas susceptibles de recibir el daño que el agente podría hacer en el entorno? Puede no ser factible identificar y penalizar cada posible situación. En términos generales, para un agente que opera en un entorno grande y multifacético, una función objetivo que se centra en un solo aspecto del entorno puede expresar implícitamente indiferencia sobre otros aspectos del medio ambiente. Un agente que optimice esta función objetivo podría producir grandes daños en el entorno si hacerlo le proporciona una pequeña ventaja para la tarea en cuestión.

Al igual que las funciones objetivos mal especificadas, los efectos secundarios pueden ser asociados como responsabilidad del diseñador a la hora de implementar la función objetivo correcta. Sin embargo, nos podemos dar cuenta de que estos efectos secundarios a tener en cuenta, pueden ser comunes a tareas muy diferentes, como por ejemplo derribar muebles, por lo que merece la pena solucionar el problema de manera genérica. Un enfoque exitoso puede ser transferible entre tareas, evitando uno de los mecanismos por los que se producen funciones objetivos incorrectas. Algunos enfoques para combatir estos problemas son:

\begin{itemize}
\item Definir un regularizador de impacto: si no queremos efectos secundarios, lo natural parece penalizar los cambios en el entorno. Esto se haría dándole preferencia por las formas de lograr su objetivo con efectos secundarios mínimos o dándole al agente un "presupuesto de impacto limitado".

\item Aprender un regularizador de impacto: un enfoque alternativo y más flexible, en lugar de definir un un regularizador de impacto a través de el aprendizaje de muchas tareas. Esto sería una instancia de transferencia de aprendizaje, es decir utilizar conocimientos ya aprendidos en otras tareas para facilitar el aprendizaje de una nueva.

\item Penalizar la influencia: además de no hacer cosas que tengan efectos secundarios, también podríamos hacer que el agente prefiera no colocarse en posiciones donde pueda hacer cosas que tienen efectos secundarios, aunque pudieran ser convenientes.

\item Enfoques de múltiples agentes: evitar los efectos secundarios se realiza con la intención de evitar las consecuencias negativas. Si a todos les gusta un efecto secundario, no hay necesidad de evitarlo. Lo que de verdad nos gustaría hacer es comprender a todos los demás agentes (incluido los humanos) y asegurarnos de que nuestras acciones no perjudiquen sus intereses.

\item Incertidumbre de recompensa: queremos evitar efectos secundarios imprevistos porque el medio ambiente está bien como está. En lugar de darle a un agente una función de recompensa única, podría ser incierto sobre la función de recompensa, con una distribución de probabilidad previa que refleja la propiedad de que los cambios aleatorios tienen más probabilidad de ser malos que buenos.
\end{itemize}

Por último, el diseñador puede haber especificado la función objetivo correcta de tal manera que se obtiene un comportamiento correcto, pero se producen malas decisiones debido a unos datos de entrenamiento pobres o a un modelo insuficientemente expresivo. La exploración de agentes puede causar consecuencias negativas y los algoritmos de aprendizaje automático no interpretables pueden tomar malas decisiones con entradas muy diferentes a aquellas con las que han sido entrenados.

Todos los agentes de aprendizaje autónomos necesitan participar en la exploración, tomar acciones que no parecen ideales según la información disponible, pero ayudan al agente a conocer su entorno actual. Sin embargo, la exploración puede ser peligrosa, ya que implica tomar acciones con consecuencias que el agente puede no entender bien. En entornos de juguete, como en videojuegos, las consecuencias no son importantes. Pero el mundo real puede ser más complicado. Las acciones mal elegidas pueden destruir al agente o atraparlo en un estado del que no puede salir, como por ejemplo los helicópteros robot pueden caer al suelo o causar dañas materiales. Políticas de exploración que son comunes eligen una acción al azar o ven acciones inexploradas con buenas previsiones y por tanto no intentan evitarlas. Estrategias de exploración más complejas adoptan una política de exploración coherente a largo plazo, lo que en realidad podría tener un mayor potencial de daño, ya que una mala política elegida intencionadamente puede ser mas perjudicial que simples acciones aleatorias. Sin embargo, a primera vista parece que a menudo debería ser posible predecir que acciones son peligrosas y explorar de una manera que las evite, incluso cuando no tenemos mucha información sobre el medio ambiente.

En la práctica, se pueden evitar estos problemas simplemente codificando evitar los comportamientos catastróficos. Por ejemplo un robot helicóptero podría programarse para anular su política con una secuencia codificada para evitar colisiones (como girar las hélices para ganar altitud) siempre que este demasiado cerca del suelo. Este enfoque funciona bien cuando solo hay algunas cosas que podrían salir mal, y los diseñadores las conocen todas con anticipación. Pero como agentes, volverse mas autónomos y actuar en dominios más complejos, puede ser cada vez mas difícil anticipar cualquier posible falla catastrófica como un agente ejecutando una red eléctrica o una operación de búsqueda y rescate. La codificación dura contra todo lo posible hace poco probable que el fracaso sea factible en estos casos, por lo que un enfoque basado en principios para prevenir en la exploración perjudicial parece esencial. Incluso en casos como el helicóptero robot, un enfoque basado en principios simplificaría el diseño del sistema y reduciría la necesidad de ingeniería específica del dominio.

Hay una literatura considerable sobre exploración segura, pero aquí describiremos rutas generales que ha tomado esta investigación.

\begin{itemize}
\item Criterios de rendimiento sensibles al riesgo: en algunos libros considera cambiar los criterios de optimización de la recompensa total esperada a otros objetivos que son mejores en prevención de eventos extraños y peligrosos. Estos enfoques implican tener el mejor comportamiento posible en el peor de los casos, o asegurar que la probabilidad de un muy mal desempeño sea pequeña.

\item Demostraciones de uso: la exploración es necesaria para que el agente encuentre los estados que son necesarios para un rendimiento casi óptimo. Podemos evitar la necesidad de explorar en conjunto si en su lugar utilizamos RL inverso o aprendizaje de aprendizaje, donde el algoritmo de aprendizaje esta provisto de trayectorias expertas de comportamiento casi óptimo. Recientes progresos en el aprendizaje por refuerzo inverso usando redes neuronales profundas para aprender la función de coste o política sugiere que podría ser posible reducir la necesidad de exploración en sistemas avanzados RL por entrenamiento en un conjunto pequeño de demostraciones. Tales demostraciones podrían ser usadas para crear una política de referencia.

\item Exploración simulada: cuanto más podamos hacer nuestra exploración en entornos simulados, en lugar de en el mundo real, hay menos oportunidades de producir daños. Probablemente siempre será necesario hacer exploraciones en el mundo real, ya que muchas situaciones complejas no pueden ser reflejadas por un simulador, pero podría ser posible aprender sobre el peligro en simulación y luego adoptar una política mas conservadora "\emph{exploración segura}" cuando actúe en el mundo real. El entrenamiento de agentes RL en entornos simulados son ya muy comunes, por lo que los avances en la "\emph{simulación centrada en la exploración}" podrían incorporarse fácilmente a los trabajos actuales. En los sistemas qe implican un continuo aprendizaje y despliegue, pueden ser interesantes problemas de investigación asociados con la forma segura de actuar de forma incremental las políticas dadas  basadas en simulaciones.

\item Exploración limitada: si sabemos que cierta zona es segura y que incluso la peor acción dentro de ella puede recuperarse o limitarse en daños, podemos permitir al agente que explore libremente dentro de esos limites. Si tenemos un modelo, podemos predecir un paso hacia delante y preguntar si esa acción nos va a llevar fuera del espacio de estados seguros. La seguridad puede definirse como permanecer en una región  del espacio de estados tal que las acciones son reversibles, o como limitación de la probabilidad de una gran recompensa negativa para pequeños valores. Otros enfoques usan funciones de seguridad y rendimiento separadas e intentan cumplir las restricciones sobre la función de seguridad con probabilidad alta.

\item Supervisión de políticas de confianza: si tenemos una política de confianza y un modelo de entorno, puede limitar la exploración a acciones de las que la política de confianza cree que podemos recuperarnos. Esta bien bucear hacia el suelo, siempre que sepamos que podemos salir a la superficie a tiempo.

\item Supervisión humana: otra posibilidad es comprobar las acciones potencialmente inseguras con un humano. Desafortunadamente, este problema se encuentra con el problema de supervisión escalable: el agente puede necesitar realizar demasiadas exploraciones para que la supervisión humana sea práctica, o tal vez sea necesaria demasiada rapidez para que los humanos los juzguen. Un desafío clave para hacer que esto funcione es hacer que el agente sea un buen juez sobre que acciones de exploración son potencialmente peligrosas y cuales son acciones seguras que se pueden tomar.

\item Experimentos potenciales: puede ser de ayuda tener un conjunto de entornos de juguete donde un agente  puede caer presa de la exploración peligrosa, pero que haya patrones de posibilidad de catástrofe suficientes para que los agentes inteligentes puedan predecirlos y evitarlos. Hasta cierto punto estas características ya existen en competiciones autónomas de helicópteros y simulaciones de rovers de Marte.
\end{itemize}

\begin{itemize}
\item Definir un regularizador de impacto: si no queremos efectos secundarios, lo natural parece penalizar los cambios en el entorno. Esto se haría dándole preferencia por las formas de lograr su objetivo con efectos secundarios mínimos o dándole al agente un "presupuesto de impacto limitado".

\item Aprender un regularizador de impacto: un enfoque alternativo y más flexible, en lugar de definir un un regularizador de impacto a través de el aprendizaje de muchas tareas. Esto sería una instancia de transferencia de aprendizaje, es decir utilizar conocimientos ya aprendidos en otras tareas para facilitar el aprendizaje de una nueva.

\item Penalizar la influencia: además de no hacer cosas que tengan efectos secundarios, también podríamos hacer que el agente prefiera no colocarse en posiciones donde pueda hacer cosas que tienen efectos secundarios, aunque pudieran ser convenientes.

\item Enfoques de múltiples agentes: evitar los efectos secundarios se realiza con la intención de evitar las consecuencias negativas. Si a todos les gusta un efecto secundario, no hay necesidad de evitarlo. Lo que de verdad nos gustaría hacer es comprender a todos los demás agentes (incluido los humanos) y asegurarnos de que nuestras acciones no perjudiquen sus intereses.

\item Incertidumbre de recompensa: queremos evitar efectos secundarios imprevistos porque el medio ambiente está bien como está. En lugar de darle a un agente una función de recompensa única, podría ser incierto sobre la función de recompensa, con una distribución de probabilidad previa que refleja la propiedad de que los cambios aleatorios tienen más probabilidad de ser malos que buenos.
\end{itemize}

\section{Peligros intencionados. Uso malicioso de la IA}
El uso malicioso de la inteligencia artificial significa un riesgo para las personas, organizaciones y estados. Puede suponer una amenaza para la seguridad digital, física y política. Existen varias propiedades relevantes en este sentido.

Los sistemas de inteligencia artificial y el conocimiento de como diseñarlos puede ser utilizado tanto constructiva como destructivamente. Los investigadores y desarrolladores no pueden elegir qué tareas producen beneficio al automatizarse y cuáles pueden ser perjudiciales ya que en la gran mayoría de casos coexisten ambas posibilidades. Por ejemplo, un sistema que busque vulnerabilidades en el software tiene una aplicación tanto defensiva como ofensiva, y las diferencias entre las capacidades de un dron que reparta paquetes de manera automática y un dron que reparta explosivos son las mismas.

Los sistemas de inteligencia artificial suelen ser eficientes y escalables. Decimos que un sistema eficiente si una vez entrenado y desplegado puede completar cierta tarea más rápido o más barato que un humano. Escalable significa que dado un sistema que pueda completar una tarea, se pueden realizar copias del sistema que completen muchas instancias de la tarea. Un sistema de reconocimiento facial es eficiente y escalable ya que una vez desarrollado y entrenado puede ser aplicado a diferentes cámaras por mucho menor coste que el que supondría contratar analistas humanos que hicieran el mismo trabajo.

Los sistemas de inteligencia artificial incrementan el anonimato y la distancia psicológica. Muchas tareas implican comunicación con otras personas, observar y ser observado, tomar decisiones para responder a su comportamiento o estar físicamente presente con ellos. Permitir que dichas tareas sean automatizadas puede permitir a los actores que de otra manera tendría que realizarlas conservar su anonimato y distancia psicológica de las personas a las que afectan. Una persona que utiliza un sistema de arma autónoma para cometer un asesinato en lugar de una pistola evita tener que estar presente en el lugar y tener que mirar a la víctima.

Los desarrollos de IA se prestan a una rápida difusión. Mientas que para atacantes puede ser difícil obtener o reproducir el hardware asociado a los sistemas de inteligencia artificial, es mucho más fácil obtener acceso a software y descubrimientos científicos relevantes. Muchos algoritmos de inteligencia artificial son reproducidos en cuestión de días o semanas. Además la cultura del desarrollo de la inteligencia artificial se caracteriza por un gran grado de apertura. Aunque puede resultar deseable limitar la difusión de determinados desarrollos, muchas veces es difícil conseguirlo.

Los sistemas de inteligencia artificial actuales tienen vulnerabilidades no resueltas. Estas incluyen ataques de envenenamiento de datos (introduciendo datos de entrenamiento que causan que el sistema cometa errores), ejemplos adversos (entradas diseñadas para ser mal clasificadas por algoritmos de aprendizaje automático) y la explotación de fallos en el diseño de los objetivos de sistemas autónomos. Estas vulnerabilidades son distintas a las tradicionales del software y demuestran que los sistemas de inteligencia artificial pueden superar el rendimiento de los humanos en muchas tareas, pero también pueden fallar de maneras que un humano nunca lo haría.

Veamos en qué ámbitos un uso malicioso de la inteligencia artificial puede resultar dañino para las personas.

% A partir de la página 30 del artículo
\subsection{Seguridad digital}

La utilización de técnicas de inteligencia artificial en ataques contra la seguridad de sistemas informáticos es una realidad, así como su utilización para labores defensivas. Muchos sistemas tecnológicos han evolucionado para ser distrubuídos en múltiples sistemas insuficientemente mantenidos y como consecuencia inseguros. Esto aumenta las oportunidades de automatización utilizando IA. Utilizar la inteligencia artificial para la defensa puede introducir nuevos riesgos.

Las operaciones contra la ciberseguridad con cada vez más sofisticadas, con ataques a la infraestructura de estados (el gusano Stuxnet a sistemas industriales principalmente en Irán o el exploit "\emph{crash override}" a la red eléctrica ucraniana). El cibercrimen muchas veces implica un gran nivel de profesionalización y organización y dichos grupos adoptan rápidamente tecnologías emergentes.

La inteligencia artificial está muy extendida en el lado defensivo de la ciberseguridad haciendo ciertas formas de defensa como la detección de spam y malware más efectivas y escalables. Al mismo tiempo, se generan nuevos incentivos para que actores maliciosos utilicen estas nuevas técnicas para realizar ataques, tales como una mayor velocidad, menos coste de trabajo y las dificultades que implican atraer y mantener mano de obra cualificada. Hasta ahora solo se han conocido ataques mediante el uso de la IA llevados a cabo por investigadores "\emph{de sombrero blanco}". Sin embargo, esto indica también que pronto estas técnicas serán aplicadas para ataques malintencionados, si es que no lo han sido ya.

La principal preocupación concerniente a la combinación de inteligencia artificial y ciberseguridad es que la inteligencia artificial puede facilitar ataques a mayor escala y mayor número de ataques con la misma cantidad de recursos que son necesarios actualmente.  Por ejemplo, los investigadores de ZeroFox demostraron que un sistema de phishing totalmente automatizado podía crear tweets específicos basados en los intereses de un usuario obteniendo una gran cantidad de clicks en links que podián ser maliciosos. En el artículo \cite{TargetedPhishing} se desarrolla el concepto de "\emph{spam dirigido a una comunidad"} consistente en el uso de técnicas de reconocimiento y generación de lenguaje natural para seleccionar como objetivo a determinada clase de personas con una manera específica de escribir.

La adaptabilidad de los sistemas de inteligencia artificial puede cambiar la situación estratégica de la ciberseguridad, aunque no está claro cuál será su impacto en el balance ataque/defensa. Muchas organizaciones utilizan sistemas de seguridad llamados Endpoint Detection and Response (EDR), construidas como combinaciones de heurísticas y algoritmos de aprendizaje automático para proveer capacidades tales como antivirus, análisis de comportamiento y prevención de exploits contra ataques sofisticados. Estos sistemas han demostrado su eficacia contra ataques llevados a cabo por humanos, pero las investigaciones muestran que sistemas de IA podrían ser capaceas de evadirlos.

Algunos escenarios en los que la utilización de la IA supone un riesgo para la seguridad digital son:

\begin{itemize}

\item Automatización de ataques de ingeniería social. La información online de las víctimas es usada para generar automáticamente paginas web, emails o enlaces maliciosos personalizados en los que probablemente clicaran, enviado desde direcciones que se hacen pasar por sus contactos, usando un estilo de escritura parecido que imita a esos contactos.

\item Automatización del descubrimiento de vulnerabilidades. Patrones históricos de vulnerabilidades de código se utilizan para descubrir más fácilmente nuevas vulnerabilidades y crear programas para explotarlas.

\item Automatización mas sofisticada de piratería. La IA se usa para mejorar la selección del objetivo, evitar ser detectado y creatividad a la hora de reaccionar a cambios de comportamiento del objetivo.

\item Denegación de servicio similar a la humana. Imitando el comportamiento humano (por ejemplo, a través de patrones de clic de velocidad humana y navegación en el sitio web), una multitud masiva de agentes autónomos colapsa una línea de servicio, evitando el acceso de usuarios legítimos y conducir al sistema a un estado potencialmente menos seguro.

\item Automatización de tareas de servicio en ciberdelincuencia penal. Los ciberdelincuentes utilizan técnicas de IA para automatizar diversas tareas que componen su modo de ataque, como el procesamiento de pagos o diálogo con las victimas de ransomware (secuestro de datos).

\item Priorizar objetivos para ataques cibernéticos mediante el aprendizaje automático. Grandes conjuntos de datos se utilizan para identificar a las víctimas de manera más eficiente, por ejemplo, estimando la riqueza de la persona y la disposición a pagar según su comportamiento en línea.

\item Explotar la IA utilizada en aplicaciones. Los ataques de envenenamiento de datos para crear puertas traseras en modelos de aprendizaje automático personalizados.

\end{itemize}

\subsection{Seguridad física}

En esta sección vemos los riesgos que supone el uso de la inteligencia artificial en la integridad física de personas e instalaciones. Muchos de estos riesgos son los mismos que los que conlleva la utilización de sistemas electrónicos en aplicaciones armamentísticas, aunque aumentados por la capacidad añadida de los sistemas de inteligencia aritificial.

La regulación legal y la investigación en defensa han sido lentas en comparación con la proliferación de aplicaciones armamentísticas en robots. Hay pocos obstáculos que puedan impedir a un atacante suficientemente preaparado y con el software y hardware adecuando de causar una gran cantidad de daño. La utilización de robots y drones pilotados remotamente se utiliza en la actualidad en conflicos realies, y la automatización es el siguiente paso.

En el corto plazo podemos esperar que la diferencia entre potencial ofensivo y defensivo siga creciendo. El coste de un sistema defensivo es enorme comparado con el necesario para realizar ataques. Al igual que la descentralización supone un riesto para la seguridad digital, la población está repartida por todo el planeta y los recursos necesarios para su defensa son inasumibles. Es por ello que la única solución posible es la intervención del suministro de robots.

Estos son algunos escenarios concretos en los que la utilización de sistemas de inteligencia artificial supone un riesgo para la seguridad física:

\begin{itemize}

\item Reutilización terrorista de sistemas comerciales de IA. Los sistemas comerciales se utilizan de manera nociva e intencionada, como el uso de drones o vehículos autónomos para entregar explosivos y causar accidentes.

\item Dotar a personas de baja habilidad de un ataque previamente de de alta habilidad en ciertas capacidades. Automatizaciones habilitada por IA de capacidades de alta habilidad, como rifles de francotirador de largo alcance y autodirigidos, reduciendo la experiencia requerida para ejecutar ciertos tipos de ataque.

\item Mayor escala de ataques. Equipos humano-máquina usando los sistemas autónomos aumentan la cantidad de daño que los individuos o grupos pequeños pueden hacer: por ejemplo, una persona lanza un ataque con muchos drones autónomos armados.

\item Ataque de enjambre. Redes distribuidas de robots autónomos. Los sistemas, que cooperan a alta velocidad, proporcionan ubicuidad en la vigilancia para monitorear grandes áreas y grupos y ejecutar rápidamente ataques coordinados.

\end {itemize}

\subsection{Seguridad política}

Por último vemos los riesgos que implica el desarrollo de las tecnologías de inteligencia artificial en el ámbito político. La inteligencia artificial permite cambiar la naturaleza de la comunicación entre individuos, corporaciones y estados. Las tecnologías de la información están afectando a instituciones políticas en muy diversos aspectos, como por ejemplo la utilización de redes sociales en campañas electorales y protestas, y la implicación de la inteligencia artificial hace que estos cambios sean más extremos. La escalabilidad de la inteligencia artificial la hace especialmente adecuada para el control y manipulación del discurso público mediante la generación de gran cantidad de contenido persuasivo, aunque muchas veces falso. Esto puede suponer un fortalecimiento de las posturas más autoritarias.

Históricamente los grandes avances tecnológicos y la inestabilidad política han estado fuertemente ligados. Estos avances pueden cambiar la correlación de poder entre estados, así como la relación entre dirigentes y disidentes. Los ejércitos y agencias de inteligencia actuales utilizan las últimas técnicas de la información para la vigilancia, como ya se hizo con avances tecnológicos importantes en su momento como el teléfono.

Los efectos de las nuevas tecnologías no son tan claros. Un ejemplo es la utilización de las redes sociales, que permite a las grandes agencias de inteligencia el seguimiento masivo, pero también han servido como motor de las protestas más recientes. La tendencia actual, reforzada por la utilización de la inteligencia artificial, es la polarización del discurso político. Los algoritmos de machine learning utilizados por las plataformas digitales tienden a mostrar al usuario solo el contenido al que es afín. Esta dinámica probablemente solo se vea acelerada conforme los algoritmos se sofistican.

Las tecnologías de la información destacan por la facilidad de copia y transmisión. La capacidad para influir se ha visto democratizada, ya que individuos o pequeños colectivos son ahora capaces de comunicarse y afectar a grandes masas. Esto no es necesariamente favorable a la democracia, ya que favorece la difusión de información falsa o manipulada. Los intentos actuales por detectar y controlar esta difusión han resultado inefectivos.

La carrera entre producción y detección de información manipulativa ha producido y producirá importantes cambios políticos. No están claras las implicaciones a largo plazo de la utilización maliciosa de la inteligencia artificial como herramienta de manipulación masiva, aunque podemos observar la utilización actual de estos medios. La proliferación de bots en redes sociales que emiten un determinado mensaje político suplantando identidades es una técnica habitual y se ha demostrado su capacidad para afectar a las afinidades políticas de la población.

La inteligencia artificial permite también la creación de noticias falsas. La capacidad de generar imágenes y vídeos sintéticos de gran calidad ya ha sido utilizada para la propagación de este tipo de noticias. De la misma manera, la generalización de estas técnicas puede servir como excusa para denegar la validez de noticias que sí son verdaderas. Un escenario en el que se desconfía de la información y la verdad es manipulable puede ser utilizado por grupos ideológicos que se basan en la desconfianza en los sistemas políticos y medios de comunicación actuales.

La inteligencia artificial ofrece también nuevos mecanismos para el control de la población que chocan con los valores democráticos. Permite una vigilancia más selectiva y eficiente. El incremento del poder de obtención de información y sobre todo de su tratamiento mientras técnicas de big data aumentan la capacidad de priorizar la atención y monitorización a individuos concretos. La capacidad de filtración de información privada es una amenaza también creciente. La legislación actual se encuentra anticuada y no contempla estas nuevas alternativas, que permiten tanto a entidades privadas como a los propios estados la menoscavación de libertades individuales.

A continuación vemos algunas situaciones concretas en las que la utilización de la inteligencia artificial supone un ataque a la seguridad política:

\begin{itemize}

\item Uso de plataformas de vigilancia automatizadas por el estado para suprimir las discrepancias con este. Los poderes de vigilancia estatal de las naciones se extienden mediante la automatización del procesamiento de imagen y audio, permitiendo la recolección, procesamiento, y explotación de información de  inteligencia a escala masiva, para innumerables propósitos, incluida la supresión del debate.

\item Informes de noticias falsas con video y audio realistas. Los videos altamente realistas están hechos de líderes de estado que parecen hacer comentarios incendiarios que en realidad nunca hicieron.

\item Campañas de desinformación hiperpersonalizadas y automatizadas. Las personas son atacadas con personalización de mensajes para afectar su comportamiento de voto.

\item Automatizar campañas de influencia. Análisis de redes sociales habilitado por IA se aprovechan para identificar personas influyentes clave, que pueden luego ser abordado con ofertas (maliciosas) o dirigido con desinformación.

\item Ataques de denegación de información. La generación de información a gran escala impulsada por bots se aprovechan para introducir ruido en los canales de información (con información falsa o simplemente distractora), haciendo más difícil tener acceso información real.

\item Manipulación de disponibilidad de información. Plataformas de medios contienen algoritmos que son usados para conducir al usuario desde o hacia un cierto contenido para manipular su comportamiento.

\end{itemize}

\section{Consecuencias de la inteligencia artificial sobre el empleo}

La irrupción de los robots, la automatización y la inteligencia artificial ya ha comenzado a transformar el mundo del empleo en algunos sectores como el de la automoción o la industria, pero continuará haciéndolo con intensidad en los próximos años.

Según cuenta en el artículo \cite{Trabajo} Nuria Oliver, premio nacional de Informática y directora global en ciencia de datos en una gran empresa cuenta que "\emph{el impacto positivo que puede tener en la sociedad}" la inteligencia artificial "\emph{es inmenso}". Y pone como ejemplo el campo de la salud, donde las posibilidades que brindan las nuevas tecnologías en algunos terrenos como la secuenciación del genoma humano o el análisis radiológico era impensable hace apenas unos años. Los robots y la inteligencia artificial serán protagonistas de lo que se denomina la cuarta revolución industrial. "\emph{Todos los estudios anticipan una transformación radical en la que se van a destruir millones de puestos de trabajo. Pero según el Foro Económico Mundial se van a crear 58 millones de puestos de trabajo netos}". Este optimismo que abre un campo de posibilidades debe ir acompañado de una adecuada política educativa puesto que, como muy bien señala, no es lo mismo ser usuario de tecnología que entender cómo funciona, y actualmente no se está preparando a los niños para ocupar esos nuevos puestos de trabajo que serán requeridos en los próximos años. Según los expertos el empleo destruido sera el de baja cualificación, mientras que la inteligencia artificial dará oportunidades, sobre todo, para perfiles STEM especialistas en Ciencia, Tecnología, Ingeniería y Matemáticas.

El temor a la destrucción de puestos de trabajo vinculada a la introducción de las máquinas no es algo nuevo. A comienzos del siglo XIX, un movimiento (el ludismo), encabezado por artesanos ingleses, se rebeló contra los nuevos inventos que traía la industrialización, como la máquina de hilar industrial, que amenazaba con dejarles sin empleo. Los luditas destruyeron máquinas y provocaron incendios en las fábricas, pero no lograron frenar el imparable avance de la industrialización. Para David Plaza, director de Información e Innovación Tecnológica del Grupo Adecco, nos cuenta en el artículo \cite{Empleo} que el temor a la destrucción de empleo ha acompañado siempre a los avances tecnológicos, pero "\emph{aunque muchos empleos terminan desapareciendo, también surgen otros nuevos que ni siquiera habíamos imaginado}". En su opinión, "\emph{deberíamos empezar a hablar más de transformación en el empleo que de creación o destrucción de puestos de trabajo}".

Hay quien sin embargo que cree que se destruirán mas empleos de los que se crearán, y que habrá un serio problema con mucha gente que se quedará sin trabajo a la que difícilmente se podrá ubicar en otros sectores como, por ejemplo, los trabajadores del sector del transporte. Como una posible solución se plantea una renta universal, que podría ser pagada con los beneficios obtenidos de la automatización.

La inteligencia artificial es una herramienta muy potente para mejorar la sociedad y que traerá avances a veces difíciles de predecir, como los que trajo internet, pero habrá que hacerlo de manera regulada para maximizar los beneficios y minimizar los daños.

\section{Conclusiones}

La aparición de la inteligencia artificial ha supuesto un importante avance tecnológico. Las posibilidades que ofrece facilitan tareas de todos los ámbitos de la vida. Estamos ya inmersos en la cuarta revolución industrial, que tiene en la inteligencia artificial uno de sus pilares. Mediante ella se están obteniendo grandes hitos científicos y técnicos inaccesibles anteriormente y el avance de estas técnicas está por tanto ligado al avance de nuestra sociedad a partir de ahora.

La preocupación por las implicaciones éticas que la inteligencia artificial y su desarrollo suscitan aparece en el mismo momento en el que aparece la disciplina. Al tratarse de un acercamiento tecnológico a características biológicas y humanas como el pensamiento, su desarrollo produce una profunda reflexión sobre estas realidades. El conocimiento sobre el funcionamiento del cerebro está ligado al desarrollo de este campo y la investigación sobre inteligencia artificial nos ayuda a comprender nuestra propia inteligencia. Existe una relación simbiótica entre filosofía, ciencia y tecnología entorno a su avance.

Es imprescindible que reflexionemos sobre las implicaciones que la inteligencia artificial y todo lo que esta permite tiene sobre nuestra sociedad. Al tratarse de una ciencia novedosa, con menos de 100 años de historia, y poco explorada, las posibilidades que ofrece son desconocidas. Esto nos lleva a pensar en lo prometedor de su desarrollo para todos los ámbitos de la vida humana pero también en las consecuencias inesperadas que puedan surgir. Debemos discernir qué avances son admisibles y beneficiosos y qué cambios pueden resultar perniciosos.

La legislación actual se encuentra muy por detrás de la realidad de este área. Un esfuerzo conjunto de legisladores y técnicos se hace acuciantemente necesario para dirigir su desarrollo y evitar posibles riesgos irreparables para nuestra sociedad. Muestras de estos riesgos se presentan ante nuestros ojos en estos momentos y eso puede significar solo una pequeña muestra del potencial peligro que implica.

La inteligencia artificial implica transformaciones en nuestra sociedad. Muchas de las tareas que hoy en día solo pueden ser realizadas por humanos serán pronto automatizadas. Esto debe traducirse en una sociedad más igualitaria en la que la carga de trabajo para las personas sea menor. Las transformaciones necesarias para que esto así sea deben realizarse en el corto plazo. De esta manera los beneficios que la tecnología nos traerá serán mucho mayores que sus desventajas.

\newpage

%Bibliographic references
\begin{thebibliography}{9}

\bibitem{RussellNorvig} \textit{Inteligencia Artificial: Un enfoque Moderno} Stuart J. Russell y Peter Norvig (1995)
\bibitem{Turing}\textit{Computing Machinery and Intelligence} Alan Turing (1950)
\bibitem{AISafety} \textit{Concrete Problems in AI Safety} Dario Amodei, Crhis Olah, Jacob Steinahrdt, Paul Christiano, John Schulman, Dan Mané (2016)
\\\texttt{https://arxiv.org/pdf/1606.06565.pdf}
\bibitem{FutureSuffering} \textit{Artificial Intelligence and Its Implications for Future Suffering} Brian Tomasik (2014)
\\\texttt{https://foundational-research.org/artificial-intelligence-and-its}
\\\texttt{-implications-for-future-suffering}
\bibitem{MaliciousUse} \textit{The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation}Miles Brundage, Shahar Avin,  Jack Clark, Helen Toner, Peter Eckersley, Ben Garfinkel, Allan Dafoe, Paul Scharre, Thomas Zeitzoff, Bobby Filar, Hyrum Anderson, Heather Roff, Gregory C. Allen, Jacob Steinhardt, Carrick Flynn, Seán Ó hÉigeartaigh, Simon Beard, Haydn Belfield, Sebastian Farquhar, Clare Lyle, Rebecca Crootof, Owain Evans, Michael Page, Joanna Bryson, Roman Yampolskiy y Dario Amodei (2018)
\\\texttt{https://img1.wsimg.com/blobby/go/3d82daa4-97fe-4096-9c6b-376b92c619de/}
\\\texttt{downloads/MaliciousUseofAI.pdf?ver=1553030594217}
\bibitem{TargetedPhishing} \textit{Community Targeted Phishing: A Middle Ground Between Mssive and Spear Phishing through Natural Language Generation} Giaretta, Alberto and Dragoni y Nicola (2017)
\\\texttt{https://www.researchgate.net/publication/319271846\_Community\_Targeted\_}
\\\texttt{Phishing\_A\_Middle\_Ground\_Between\_Massive\_and\_Spear\_Phishing\_}
\texttt{through\_Natural\_Language\_Generation}
\bibitem{Speculations} \textit{Speculations Concerning the First Ultraintelligent Machine} Irving John Good (1963)
\\\texttt{https://www.sciencedirect.com/science/article/pii/S0065245808604180\#!}
\bibitem{Empleo} \textit{No temas a los robots, tu empleo está a salvo si sigues formándote} Isabel Munera (2016)
\\\texttt{https://lab.elmundo.es/inteligencia-artificial/trabajo-robots.html}
\bibitem{Trabajo} \textit{La inteligencia artificial creará 58 millones de puestos de trabajo} Azahara Mígel y Maruxa Ruiz del Árbol (2019)
\\\texttt{https://elfuturoesapasionante.elpais.com/nuria-oliver-la-inteligencia-}
\\\texttt{artificial-creara-58-millones-de-puestos-de-trabajo/}
\bibitem{Depender} \textit{"La inteligencia artificial será como la electricidad. Todo dependerá de ella"} Agathe Cortes (2019)
\\\texttt{https://elpais.com/tecnologia/2019/11/27/actualidad/1574872985\_952092.html}
\bibitem{Asimov} \textit{The Foundation Series} Isaac Asimov (1942-1957,1982-1992)
\bibitem{Sprawl} \textit{Sprawl trilogy} William Gibson (1984,1986,1988)
\bibitem{Robots} \textit{Fables for Robots} Bajki Robotów (1964)
\bibitem{Maquinas} \textit{Maquinas como yo} Ian McEwan(2019)
\bibitem{Androides} \textit{Do Androids Dream of Electric Sheep?} Philip K. Dick (1968)
\bibitem{Shell} \textit{Ghost in the Shell} Masamune Shirow (1989)
\bibitem{Google} \textit{Neural Architecture Search with Reinforcement Learning} Barret Zoph y Quoc V. Le (2017)
\\\texttt{https://ai.google/research/pubs/pub45826}
\end{thebibliography}

\end{document}
